{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test function for automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column transformer to handle different types of features\n",
    "text_featues = ['review_stemmed_nostop']\n",
    "num_features = ['drugName_encoded','condition_encoded', 'usefulCount','day', 'month', 'year',\n",
    "                   'count_word', 'count_unique_word', 'count_letters',\n",
    "                   'count_punctuations', 'count_words_upper', 'count_words_title',\n",
    "                   'count_stopwords', 'mean_word_len']\n",
    "\n",
    "# Define column transformer to handle text and numeric features separately\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_features),\n",
    "    ('text_cv', CountVectorizer(), 'review_stemmed_nostop'),\n",
    "    ('text_tfidf', TfidfVectorizer(), 'review_stemmed_nostop')\n",
    "])\n",
    "\n",
    "# Define pipeline\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DummyClassifier(strategy='stratified'))\n",
    "])\n",
    "\n",
    "# Define hyperparameters to test\n",
    "params = {\n",
    "    'preprocessor__text_cv__ngram_range': [(1,2), (1,3), (1,4), (1,5)],\n",
    "    'preprocessor__text_tfidf__ngram_range': [(1,2), (1,3), (1,4), (1,5)],\n",
    "    'classifier__strategy': ['stratified', 'most_frequent']\n",
    "}\n",
    "\n",
    "# Define cross-validation settings\n",
    "cv = 5\n",
    "scoring = 'f1'\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "results = []\n",
    "for param_set in ParameterGrid(params):\n",
    "    pipe.set_params(**param_set)\n",
    "    score = np.mean(cross_val_score(pipe, X_train, y_train, cv=cv, scoring=scoring))\n",
    "    results.append((score, param_set))\n",
    "results.sort(reverse=True)\n",
    "\n",
    "# Print train and test scores for best parameter set\n",
    "best_params = results[0][1]\n",
    "pipe.set_params(**best_params)\n",
    "pipe.fit(X_train, y_train)\n",
    "train_score = f1_score(y_train, pipe.predict(X_train))\n",
    "test_score = f1_score(y_test, pipe.predict(X_test))\n",
    "print(f'Train score: {train_score:.4f}')\n",
    "print(f'Test score: {test_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that evaluates a given model on a dataset and returns the test and train accuracies \n",
    "# GridSearchCV is implemented to find the optimal hyperparameters\n",
    "\n",
    "def eval_model(model, name, param_grid, scaler = None, _X_train = X_train, _X_test = X_test, _y_train = y_train, _y_test = y_test):\n",
    "    # Define column transformer to handle different types of features\n",
    "    num_features = ['drugName_encoded','condition_encoded', 'usefulCount','day', 'month', 'year',\n",
    "                    'count_word', 'count_unique_word', 'count_letters',\n",
    "                    'count_punctuations', 'count_words_upper', 'count_words_title',\n",
    "                    'count_stopwords', 'mean_word_len']\n",
    "    #txt_featues = 'review_stemmed_nostop'\n",
    "    \n",
    "    # If a scaler is provided, scale the training and test data\n",
    "    if scaler:\n",
    "        _X_train = scaler.fit_transform(_X_train)\n",
    "        _X_test = scaler.transform(_X_test)\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_features),\n",
    "    #('text_cv', CountVectorizer(), txt_features),\n",
    "    ('text_tfidf', TfidfVectorizer(), 'review_stemmed_nostop')\n",
    "    ])\n",
    "    \n",
    "    # Define the pipeline with the desired preprocessing steps and model\n",
    "    pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    model\n",
    "    ])\n",
    "\n",
    "    # Use GridSearchCV to find the best hyperparameters for the model\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "    grid_search.fit(_X_train, _y_train)\n",
    "    \n",
    "    # Print the name of the model and the best hyperparameters\n",
    "    print(name, \":\")\n",
    "    print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "    \n",
    "    # Evaluate the model on the test and training data using the best hyperparameters\n",
    "    test_accuracy = grid_search.score(_X_test, _y_test)\n",
    "    train_accuracy = grid_search.score(_X_train, _y_train)\n",
    "    \n",
    "    # Print the accuracies\n",
    "    print(f'Training Accuracy: {train_accuracy}')\n",
    "    print(f'Test Accuracy: {test_accuracy}\\n')\n",
    "    \n",
    "    # Return the test and train accuracies\n",
    "    return (test_accuracy, train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the models to evaluate\n",
    "models = {\n",
    "    \"lr\": LogisticRegression(max_iter=10_000),\n",
    "    \"nb\": MultinomialNB(),\n",
    "    \"rfc\": RandomForestClassifier()#,\n",
    "    #\"MLP\": MLPClassifier(max_iter=10_000)\n",
    "}\n",
    "\n",
    "# Define the hyperparameter grids for each model\n",
    "param_grids = {\n",
    "    \"preprocessor__text_tfidf\": { \"ngram_range\": [(1,2), (1,3), (1,4), (1,5)]},\n",
    "    \"lr\": {\"C\": [0.01, 0.1, 1, 10]},\n",
    "    \"nb\": {\"alpha\": [0.01, 0.1, 1.0]},\n",
    "    \"rfc\": {\"n_estimators\": [10, 50, 100]}#,\n",
    "   # \"MLP\": {\"hidden_layer_sizes\": [(10,), (20,), (30,)]}\n",
    "}\n",
    "\n",
    "# Evaluate each model using eval_model()\n",
    "test_accs = []\n",
    "train_accs = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    test_acc, train_acc = eval_model(model, name, param_grid=param_grids[name], _X_train=X_train, _X_test=X_test, _y_train=y_train, _y_test=y_test,num_features, txt_featues)\n",
    "    test_accs.append(test_acc)\n",
    "    train_accs.append(train_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the train and test accuracies for all models in a bar chart\n",
    "labels = list(models.keys())\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, train_accs, width, label='Train Accuracy')\n",
    "rects2 = ax.bar(x + width/2, test_accs, width, label='Test Accuracy')\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Train and Test Accuracies for Different Models')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - Dummy Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline for initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train model - Initial\n",
    "# Pipeline for initial model before hyperparameter tuning\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    #('text_tfidf', TfidfVectorizer(), 'review_stemmed_nostop')\n",
    "    ('text_cv', CountVectorizer(), 'review_stemmed_nostop')\n",
    "    ])\n",
    "\n",
    "# Define the models to evaluate\n",
    "model = DummyClassifier()\n",
    "\n",
    "# Define the pipeline with the desired preprocessing steps and model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Evaluate the pipeline on the test data\n",
    "train_score = pipeline.score(X_train_balanced, y_train_balanced)\n",
    "test_score = pipeline.score(X_test, y_test)\n",
    "\n",
    "\n",
    "# Print the scores\n",
    "print(f'Train score: {train_score:.4f}')\n",
    "print(f'Test score: {test_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation\n",
    "# use the pipeline to make predictions on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# print the classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('text_tfidf', TfidfVectorizer(), 'review_stemmed_nostop')\n",
    "    ])\n",
    "\n",
    "# Define the models to evaluate\n",
    "model = DummyClassifier()\n",
    "\n",
    "# Define the pipeline with the desired preprocessing steps and model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline on the test data\n",
    "train_score = pipeline.score(X_train, y_train)\n",
    "test_score = pipeline.score(X_test, y_test)\n",
    "\n",
    "\n",
    "# Print the scores\n",
    "print(f'Train score: {train_score:.4f}')\n",
    "print(f'Test score: {test_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation \n",
    "\n",
    "# use the pipeline to make predictions on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# print the classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    ('text_tfidf', TfidfVectorizer(), 'review_stemmed_nostop')\n",
    "    ])\n",
    "\n",
    "# Define the models to evaluate\n",
    "models = DummyClassifier()\n",
    "\n",
    "# Define the pipeline with the desired preprocessing steps and model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DummyClassifier())\n",
    "])\n",
    "\n",
    "# Define hyperparameters to test\n",
    "params = {\n",
    "    'preprocessor__text_tfidf__ngram_range': [(1,2), (1,3), (1,4), (1,5)],\n",
    "    'classifier__strategy': ['stratified', 'most_frequent', 'prior', 'uniform']\n",
    "}\n",
    "\n",
    "# Define cross-validation settings\n",
    "cv = 5\n",
    "scoring = 'f1'\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters for the model\n",
    "grid_search = GridSearchCV(pipeline, param_grid=params, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the name of the model and the best hyperparameters\n",
    "print(\"Dummy Classifier:\")\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test and training data using the best hyperparameters\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "train_score = grid_search.score(X_train, y_train)\n",
    "\n",
    "# Print the accuracies\n",
    "print(f'Train score: {train_score:.4f}')\n",
    "print(f'Test score: {test_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GridsearchCV for hyperparameter tuning\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('text_tfidf', TfidfVectorizer(), 'review_stemmed_nostop')\n",
    "    ])\n",
    "\n",
    "# Define the models to evaluate\n",
    "models = DummyClassifier()\n",
    "\n",
    "# Define the pipeline with the desired preprocessing steps and model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DummyClassifier())\n",
    "])\n",
    "\n",
    "# Define hyperparameters to test\n",
    "params = {\n",
    "    'preprocessor__text_tfidf__ngram_range': [(1,2), (1,3), (1,4), (1,5)],\n",
    "    'classifier__strategy': ['stratified', 'most_frequent', 'prior', 'uniform']\n",
    "}\n",
    "\n",
    "# Define cross-validation settings\n",
    "cv = 5\n",
    "scoring = 'f1'\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters for the model\n",
    "grid_search = GridSearchCV(pipeline, param_grid=params, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Print the name of the model and the best hyperparameters\n",
    "print(\"Dummy Classifier:\")\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model evaluation\n",
    "# Evaluate the model on the test and training data using the best hyperparameters\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "train_score = grid_search.score(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Print the accuracies\n",
    "print(f'Train score: {train_score:.4f}')\n",
    "print(f'Test score: {test_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
