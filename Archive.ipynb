{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test function for automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column transformer to handle different types of features\n",
    "text_featues = ['review_stemmed_nostop']\n",
    "num_features = ['drugName_encoded','condition_encoded', 'usefulCount','day', 'month', 'year',\n",
    "                   'count_word', 'count_unique_word', 'count_letters',\n",
    "                   'count_punctuations', 'count_words_upper', 'count_words_title',\n",
    "                   'count_stopwords', 'mean_word_len']\n",
    "\n",
    "# Define column transformer to handle text and numeric features separately\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_features),\n",
    "    ('text_cv', CountVectorizer(), 'review_stemmed_nostop'),\n",
    "    ('text_tfidf', TfidfVectorizer(), 'review_stemmed_nostop')\n",
    "])\n",
    "\n",
    "# Define pipeline\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DummyClassifier(strategy='stratified'))\n",
    "])\n",
    "\n",
    "# Define hyperparameters to test\n",
    "params = {\n",
    "    'preprocessor__text_cv__ngram_range': [(1,2), (1,3), (1,4), (1,5)],\n",
    "    'preprocessor__text_tfidf__ngram_range': [(1,2), (1,3), (1,4), (1,5)],\n",
    "    'classifier__strategy': ['stratified', 'most_frequent']\n",
    "}\n",
    "\n",
    "# Define cross-validation settings\n",
    "cv = 5\n",
    "scoring = 'f1'\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "results = []\n",
    "for param_set in ParameterGrid(params):\n",
    "    pipe.set_params(**param_set)\n",
    "    score = np.mean(cross_val_score(pipe, X_train, y_train, cv=cv, scoring=scoring))\n",
    "    results.append((score, param_set))\n",
    "results.sort(reverse=True)\n",
    "\n",
    "# Print train and test scores for best parameter set\n",
    "best_params = results[0][1]\n",
    "pipe.set_params(**best_params)\n",
    "pipe.fit(X_train, y_train)\n",
    "train_score = f1_score(y_train, pipe.predict(X_train))\n",
    "test_score = f1_score(y_test, pipe.predict(X_test))\n",
    "print(f'Train score: {train_score:.4f}')\n",
    "print(f'Test score: {test_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that evaluates a given model on a dataset and returns the test and train accuracies \n",
    "# GridSearchCV is implemented to find the optimal hyperparameters\n",
    "\n",
    "def eval_model(model, name, param_grid, scaler = None, _X_train = X_train, _X_test = X_test, _y_train = y_train, _y_test = y_test):\n",
    "    # Define column transformer to handle different types of features\n",
    "    num_features = ['drugName_encoded','condition_encoded', 'usefulCount','day', 'month', 'year',\n",
    "                    'count_word', 'count_unique_word', 'count_letters',\n",
    "                    'count_punctuations', 'count_words_upper', 'count_words_title',\n",
    "                    'count_stopwords', 'mean_word_len']\n",
    "    #txt_featues = 'review_stemmed_nostop'\n",
    "    \n",
    "    # If a scaler is provided, scale the training and test data\n",
    "    if scaler:\n",
    "        _X_train = scaler.fit_transform(_X_train)\n",
    "        _X_test = scaler.transform(_X_test)\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_features),\n",
    "    #('text_cv', CountVectorizer(), txt_features),\n",
    "    ('text_tfidf', TfidfVectorizer(), 'review_stemmed_nostop')\n",
    "    ])\n",
    "    \n",
    "    # Define the pipeline with the desired preprocessing steps and model\n",
    "    pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    model\n",
    "    ])\n",
    "\n",
    "    # Use GridSearchCV to find the best hyperparameters for the model\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "    grid_search.fit(_X_train, _y_train)\n",
    "    \n",
    "    # Print the name of the model and the best hyperparameters\n",
    "    print(name, \":\")\n",
    "    print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "    \n",
    "    # Evaluate the model on the test and training data using the best hyperparameters\n",
    "    test_accuracy = grid_search.score(_X_test, _y_test)\n",
    "    train_accuracy = grid_search.score(_X_train, _y_train)\n",
    "    \n",
    "    # Print the accuracies\n",
    "    print(f'Training Accuracy: {train_accuracy}')\n",
    "    print(f'Test Accuracy: {test_accuracy}\\n')\n",
    "    \n",
    "    # Return the test and train accuracies\n",
    "    return (test_accuracy, train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the models to evaluate\n",
    "models = {\n",
    "    \"lr\": LogisticRegression(max_iter=10_000),\n",
    "    \"nb\": MultinomialNB(),\n",
    "    \"rfc\": RandomForestClassifier()#,\n",
    "    #\"MLP\": MLPClassifier(max_iter=10_000)\n",
    "}\n",
    "\n",
    "# Define the hyperparameter grids for each model\n",
    "param_grids = {\n",
    "    \"preprocessor__text_tfidf\": { \"ngram_range\": [(1,2), (1,3), (1,4), (1,5)]},\n",
    "    \"lr\": {\"C\": [0.01, 0.1, 1, 10]},\n",
    "    \"nb\": {\"alpha\": [0.01, 0.1, 1.0]},\n",
    "    \"rfc\": {\"n_estimators\": [10, 50, 100]}#,\n",
    "   # \"MLP\": {\"hidden_layer_sizes\": [(10,), (20,), (30,)]}\n",
    "}\n",
    "\n",
    "# Evaluate each model using eval_model()\n",
    "test_accs = []\n",
    "train_accs = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    test_acc, train_acc = eval_model(model, name, param_grid=param_grids[name], _X_train=X_train, _X_test=X_test, _y_train=y_train, _y_test=y_test,num_features, txt_featues)\n",
    "    test_accs.append(test_acc)\n",
    "    train_accs.append(train_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the train and test accuracies for all models in a bar chart\n",
    "labels = list(models.keys())\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, train_accs, width, label='Train Accuracy')\n",
    "rects2 = ax.bar(x + width/2, test_accs, width, label='Test Accuracy')\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Train and Test Accuracies for Different Models')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - Dummy Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline for initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train model - Initial\n",
    "# Pipeline for initial model before hyperparameter tuning\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    #('text_tfidf', TfidfVectorizer(), 'review_stemmed_nostop')\n",
    "    ('text_cv', CountVectorizer(), 'review_stemmed_nostop')\n",
    "    ])\n",
    "\n",
    "# Define the models to evaluate\n",
    "model = DummyClassifier()\n",
    "\n",
    "# Define the pipeline with the desired preprocessing steps and model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Evaluate the pipeline on the test data\n",
    "train_score = pipeline.score(X_train_balanced, y_train_balanced)\n",
    "test_score = pipeline.score(X_test, y_test)\n",
    "\n",
    "\n",
    "# Print the scores\n",
    "print(f'Train score: {train_score:.4f}')\n",
    "print(f'Test score: {test_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation\n",
    "# use the pipeline to make predictions on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# print the classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('text_tfidf', TfidfVectorizer(), 'review_stemmed_nostop')\n",
    "    ])\n",
    "\n",
    "# Define the models to evaluate\n",
    "model = DummyClassifier()\n",
    "\n",
    "# Define the pipeline with the desired preprocessing steps and model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline on the test data\n",
    "train_score = pipeline.score(X_train, y_train)\n",
    "test_score = pipeline.score(X_test, y_test)\n",
    "\n",
    "\n",
    "# Print the scores\n",
    "print(f'Train score: {train_score:.4f}')\n",
    "print(f'Test score: {test_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## arkiv\n",
    "## Defining funciton for pipeline\n",
    "# The function evaluates a given model on a dataset and returns the test and train scores \n",
    "\n",
    "def model_pipeline(model, text_processor=None, num_processor=None,_X_train = X_train, _X_test = X_test, _y_train = y_train, _y_test = y_test):\n",
    "    # Define column transformer to handle different types of features\n",
    "    num_features = ['drugName_encoded','condition_encoded', 'usefulCount','day', 'month', 'year',\n",
    "                    'count_word', 'count_unique_word', 'count_letters',\n",
    "                    'count_punctuations', 'count_words_upper', 'count_words_title',\n",
    "                    'count_stopwords', 'mean_word_len']\n",
    "    ##1\n",
    "    # if text_processor or num_processor:\n",
    "    #     preprocessor = ColumnTransformer([\n",
    "    #         ('num', num_processor, num_features),\n",
    "    #         #('text_cv', CountVectorizer(), txt_features),\n",
    "    #         ('text', text_processor, 'review_stemmed_nostop')\n",
    "    #         ])\n",
    "    # else: \n",
    "    #     preprocessor = FunctionTransformer(lambda x: x, validate=False)\n",
    "    \n",
    "    ## 2 - virker\n",
    "    # preprocessor_transformers = []\n",
    "    # if num_processor:\n",
    "    #     preprocessor_transformers.append(('num', num_processor, num_features))\n",
    "    # if text_processor:\n",
    "    #     preprocessor_transformers.append(('text', text_processor, 'review_stemmed_nostop'))\n",
    "\n",
    "    # if preprocessor_transformers:\n",
    "    #     preprocessor = ColumnTransformer(preprocessor_transformers)\n",
    "    # else:\n",
    "    #     preprocessor = FunctionTransformer(lambda x: x, validate=False)\n",
    "\n",
    "    # 3 test\n",
    "    preprocessor_transformers = []\n",
    "    if num_processor:\n",
    "        preprocessor_transformers.append(('num', num_processor, num_features))\n",
    "    if text_processor:\n",
    "        preprocessor_transformers.append(('text', text_processor, 'review_stemmed_nostop'))\n",
    "        \n",
    "    preprocessing = ColumnTransformer(preprocessor_transformers) if preprocessor_transformers else None\n",
    "\n",
    "    print(preprocessor_transformers)\n",
    "    print(bool(num_processor))\n",
    "    print(bool(text_processor))\n",
    "    print(preprocessing)\n",
    "\n",
    "\n",
    "    # Define the pipeline with the desired preprocessing steps and model\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model),\n",
    "        ])\n",
    "\n",
    "    # Train model - fit pipeline to training set\n",
    "    pipeline.fit(_X_train, _y_train)\n",
    "\n",
    "    #  Predict y in test set\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    # Evaluate the model on the test and training data u\n",
    "    train_score = pipeline.score(_X_train, _y_train)\n",
    "    test_score = pipeline.score(_X_test, _y_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Print the accuracies\n",
    "    print(f'Train score: {train_score:.4f}')\n",
    "    print(f'Test score: {test_score:.4f}\\n')\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "\n",
    "    # Return the test and train accuracies\n",
    "    return (test_score, train_score, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation \n",
    "\n",
    "# use the pipeline to make predictions on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# print the classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## udklip\n",
    "    preprocessor_transformers = []\n",
    "    if num_processor:\n",
    "        preprocessor_transformers.append(('num', num_processor, num_features))\n",
    "    if text_processor:\n",
    "        preprocessor_transformers.append(('text', text_processor, 'review_stemmed_nostop'))\n",
    "\n",
    "    # if preprocessor_transformers:\n",
    "    #     preprocessor = ColumnTransformer(preprocessor_transformers)\n",
    "    # else:\n",
    "    #     preprocessor = FunctionTransformer(lambda x: x, validate=False)\n",
    "        \n",
    "    #preprocessing = ColumnTransformer(preprocessor_transformers) if preprocessor_transformers else 'passthrough'\n",
    "\n",
    "    print(preprocessor_transformers)\n",
    "    print(bool(num_processor))\n",
    "    print(bool(text_processor))\n",
    "  \n",
    "\n",
    "    if preprocessor_transformers: \n",
    "        # Define the pipeline with the desired preprocessing steps and model\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', ColumnTransformer(preprocessor_transformers)),\n",
    "            ('classifier', model),\n",
    "            ])\n",
    "    else:\n",
    "          pipeline = Pipeline([\n",
    "            ('classifier', model),\n",
    "            ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    ('text_tfidf', TfidfVectorizer(), 'review_stemmed_nostop')\n",
    "    ])\n",
    "\n",
    "# Define the models to evaluate\n",
    "models = DummyClassifier()\n",
    "\n",
    "# Define the pipeline with the desired preprocessing steps and model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DummyClassifier())\n",
    "])\n",
    "\n",
    "# Define hyperparameters to test\n",
    "params = {\n",
    "    'preprocessor__text_tfidf__ngram_range': [(1,2), (1,3), (1,4), (1,5)],\n",
    "    'classifier__strategy': ['stratified', 'most_frequent', 'prior', 'uniform']\n",
    "}\n",
    "\n",
    "# Define cross-validation settings\n",
    "cv = 5\n",
    "scoring = 'f1'\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters for the model\n",
    "grid_search = GridSearchCV(pipeline, param_grid=params, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the name of the model and the best hyperparameters\n",
    "print(\"Dummy Classifier:\")\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test and training data using the best hyperparameters\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "train_score = grid_search.score(X_train, y_train)\n",
    "\n",
    "# Print the accuracies\n",
    "print(f'Train score: {train_score:.4f}')\n",
    "print(f'Test score: {test_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GridsearchCV for hyperparameter tuning\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('text_tfidf', TfidfVectorizer(), 'review_stemmed_nostop')\n",
    "    ])\n",
    "\n",
    "# Define the models to evaluate\n",
    "models = DummyClassifier()\n",
    "\n",
    "# Define the pipeline with the desired preprocessing steps and model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DummyClassifier())\n",
    "])\n",
    "\n",
    "# Define hyperparameters to test\n",
    "params = {\n",
    "    'preprocessor__text_tfidf__ngram_range': [(1,2), (1,3), (1,4), (1,5)],\n",
    "    'classifier__strategy': ['stratified', 'most_frequent', 'prior', 'uniform']\n",
    "}\n",
    "\n",
    "# Define cross-validation settings\n",
    "cv = 5\n",
    "scoring = 'f1'\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters for the model\n",
    "grid_search = GridSearchCV(pipeline, param_grid=params, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Print the name of the model and the best hyperparameters\n",
    "print(\"Dummy Classifier:\")\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model evaluation\n",
    "# Evaluate the model on the test and training data using the best hyperparameters\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "train_score = grid_search.score(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Print the accuracies\n",
    "print(f'Train score: {train_score:.4f}')\n",
    "print(f'Test score: {test_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_numerical_features_array, val_categorical_features_dict, val_text_features_array, val_labels_array = prepare_data(X_val_bert_balanced, y_val_bert_balanced)\n",
    "\n",
    "# Evaluate the final model on the validation set\n",
    "model_bert.evaluate(\n",
    "    {\n",
    "        'numerical': val_numerical_features_array,\n",
    "        'drug_input': val_categorical_features_dict['drugName'],\n",
    "        'condition_input': val_categorical_features_dict['condition'],\n",
    "        'text': val_text_features_array\n",
    "    }, val_labels_array,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    x=[        test_numerical_features,        test_categorical_features['condition'],\n",
    "        test_categorical_features['drugName'],\n",
    "        test_text_features\n",
    "    ],\n",
    "    y=y_test\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Dense, concatenate\n",
    "\n",
    "# # Define input layers\n",
    "# text_input = Input(shape=(None,), dtype=tf.string, name='text_input')\n",
    "# numeric_input = Input(shape=(6,), name='numeric_input') # assuming 4 numeric features\n",
    "\n",
    "# # Define embedding layer for text input\n",
    "# embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length)(text_input)\n",
    "\n",
    "# # Define dense layer for numerical input\n",
    "# dense_layer = Dense(8, activation='relu')(numeric_input)\n",
    "\n",
    "# # Concatenate the two layers\n",
    "# concat_layer = concatenate([embedding_layer, dense_layer])\n",
    "\n",
    "# # Define output layer\n",
    "# output_layer = Dense(1, activation='sigmoid')(concat_layer)\n",
    "\n",
    "# # Define the model\n",
    "# model = tf.keras.models.Model(inputs=[text_input, numeric_input], outputs=output_layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## chatGPT\n",
    "# # Load BERT model and preprocessing module\n",
    "# bert_model = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3')\n",
    "# bert_prep = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
    "\n",
    "# # Load data\n",
    "# df = pd.read_csv('reviews.csv')\n",
    "\n",
    "# # Preprocess categorical features\n",
    "# cat_cols = ['drugName', 'condition']\n",
    "# for col in cat_cols:\n",
    "#     df[col] = pd.Categorical(df[col])\n",
    "#     df[col] = df[col].cat.codes\n",
    "\n",
    "# # Preprocess numerical features\n",
    "# num_cols = ['usefulCount', 'day', 'month', 'year']\n",
    "# for col in num_cols:\n",
    "#     df[col] = (df[col] - df[col].mean()) / df[col].std()\n",
    "\n",
    "# # Concatenate features\n",
    "# X = df[['drugName', 'condition', 'usefulCount', 'day', 'month', 'year', 'review_clean']]\n",
    "# y = df['sentiment']\n",
    "\n",
    "# X_cat = tf.keras.layers.Input(shape=(2,), dtype=tf.int32, name='categorical')\n",
    "# X_num = tf.keras.layers.Input(shape=(4,), dtype=tf.float32, name='numerical')\n",
    "# X_text = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "\n",
    "# # Preprocess input text using BERT preprocess layer\n",
    "# text_prep = bert_prep(X_text)\n",
    "\n",
    "# # Concatenate categorical, numerical, and text features\n",
    "# X_cat_emb = tf.keras.layers.Embedding(input_dim=5000, output_dim=10)(X_cat)\n",
    "# X_cat_flat = tf.keras.layers.Flatten()(X_cat_emb)\n",
    "# X_num_dense = tf.keras.layers.Dense(10, activation='relu')(X_num)\n",
    "# X_text_enc = bert_model(text_prep)['pooled_output']\n",
    "# X_concat = tf.keras.layers.concatenate([X_cat_flat, X_num_dense, X_text_enc], axis=-1)\n",
    "\n",
    "# # Define dropout and output layers for NN\n",
    "# dropout_layer = tf.keras.layers.Dropout(0.1, name='dropout')(X_concat)\n",
    "# output_layer = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(dropout_layer)\n",
    "\n",
    "# # Combine all layers to create the final model\n",
    "# model = tf.keras.Model(inputs=[X_cat, X_num, X_text], outputs=[output_layer])\n",
    "\n",
    "# # Compile the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate features\n",
    "#X = df[['drugName', 'condition', 'usefulCount', 'day', 'month', 'year', 'review_clean']]\n",
    "#y = df['sentiment']\n",
    "\n",
    "cat_input_condition = Input(shape=(1,), dtype=tf.string, name='condition_input')\n",
    "cat_input_drug = Input(shape=(1,), dtype=tf.string, name='drug_input')\n",
    "num_input = tf.keras.layers.Input(shape=(4,), dtype=tf.float32, name='numerical')\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess categorical features\n",
    "   \n",
    "# create a list of unique conditions and drug names\n",
    "# unique_conditions = df['condition'].unique().tolist()\n",
    "# unique_drugNames = df['drugName'].unique().tolist()\n",
    "vocab_condition = df['condition'].unique().tolist()\n",
    "vocab_drug = df['drugName'].unique().tolist()\n",
    "\n",
    "# define categorical feature columns for 'condition' and 'drugName'\n",
    "# cond_feature_col = tf.feature_column.categorical_column_with_vocabulary_list('condition', unique_conditions)\n",
    "# drug_feature_col = tf.feature_column.categorical_column_with_vocabulary_list('drugName', unique_drugNames)\n",
    "cat_condition = categorical_column_with_vocabulary_list(key='condition', vocabulary_list=vocab_condition)\n",
    "cat_drug = categorical_column_with_vocabulary_list(key='drugName', vocabulary_list=vocab_drug)\n",
    "\n",
    "# one-hot encode categorical feature columns\n",
    "# cond_onehot_col = tf.feature_column.indicator_column(cond_feature_col)\n",
    "# drug_onehot_col = tf.feature_column.indicator_column(drug_feature_col)\n",
    "\n",
    "# Embedding categorical columns\n",
    "encoded_condition = embedding_column(cat_condition, dimension=8)\n",
    "encoded_drug = embedding_column(cat_drug, dimension=16)\n",
    "\n",
    "# Concatenating categorical colums\n",
    "cat_cols = [encoded_condition, encoded_drug]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess numerical features (standardScaler)\n",
    "\n",
    "# StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalize numerical features\n",
    "num_cols = ['usefulCount', 'day', 'month', 'year']\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "num_cols_scaled = df[num_cols]\n",
    "# # Scale numerical features\n",
    "#num_cols_scaled = tf.keras.layers.experimental.preprocessing.Normalization()(num_input)\n",
    "\n",
    "# function to scale\n",
    "def num_scale(feature):\n",
    "  scaler = StandardScaler()\n",
    "  scaler.fit(train[[feature]])\n",
    "  def standardize(x):\n",
    "    return scaler.transform([[x]])[0][0]\n",
    "  return standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text using BERT preprocess layer\n",
    "text_prep = bert_prep(text_input)\n",
    "\n",
    "# Encode text using bert model encoder\n",
    "text_enc = bert_enc(text_prep)['pooled_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating categorical, numerical, and text features\n",
    "\n",
    "#X_concat = tf.keras.layers.concatenate([X_cat_flat, X_num_dense, X_text_enc], axis=-1)\n",
    "\n",
    "# Define dropout layer\n",
    "dropout_layer = tf.keras.layers.Dropout(0.1, name='dropout')(text_enc)\n",
    "# Concatenating categorical, numerical, and text features\n",
    "concat_layer = Concatenate()([dropout_layer, num_cols_scaled, encoded_condition, encoded_drug])\n",
    "\n",
    "# Define output layer for NN\n",
    "output_layer = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(concat_layer)\n",
    "\n",
    "# Define dropout and output layers for NN - old\n",
    "# dropout_layer = tf.keras.layers.Dropout(0.1, name='dropout')(X_concat)\n",
    "# output_layer = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(dropout_layer)\n",
    "\n",
    "# Combine all layers to create the final model\n",
    "model_bert = tf.keras.Model(inputs=[text_input, num_input, cat_input_condition, cat_input_drug], outputs=[output_layer])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
