{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "- https://www.kaggle.com/code/sumitm004/eda-and-sentiment-analysis\n",
    "\n",
    "Overskuelig og m√•ske applicable til vores projekt\n",
    "- https://www.kaggle.com/code/chocozzz/recommendation-medicines-by-using-a-review/notebook#3.-Model\n",
    "\n",
    "\n",
    "- https://www.kaggle.com/code/neilash/team-ndl-algorithms-and-illnesses/notebook\n",
    "\n",
    "\n",
    "LABS\n",
    "- LAB 4 - Model Evaluation and Improvement \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Business Understanding\n",
    "- Describe the problem and the goal of the project\n",
    "- Explain why sentiment analysis is important for drug reviews\n",
    "- Define the evaluation metrics (e.g. accuracy, precision, recall, F1 score)\n",
    "\n",
    "## Data Understanding\n",
    "- Load the dataset and inspect its features and target variable\n",
    "- Check for missing values, duplicates, and class imbalance\n",
    "- Explore the distribution of the target variable (positive/negative reviews)\n",
    "- Visualize the data (e.g. word clouds, histograms, box plots) to gain insights about the text and its characteristics\n",
    "\n",
    "## Preprocessing\n",
    "- Define a preprocessing pipeline that includes tokenization, stemming, and stop word removal\n",
    "- Apply the pipeline to the text data\n",
    "- Convert the text data into numerical data using a suitable embedding method (e.g. TF-IDF, Word2Vec, BERT)\n",
    "\n",
    "## Modeling\n",
    "- Define a set of candidate models (e.g. Logistic Regression, Naive Bayes, SVM, Random Forest, BERT)\n",
    "- Baseline models Dummy Classifier\n",
    "- Set up a pipeline that combines the preprocessing pipeline with a classifier\n",
    "- Use grid search to find the best hyperparameters for each model\n",
    "- Train each model on the training data and evaluate its performance on the validation data\n",
    "\n",
    "## Evaluation\n",
    "- Select the best performing model based on the evaluation metrics\n",
    "- Test the selected model on the test data and report its performance\n",
    "- Analyze the errors made by the model and suggest ways to improve it\n",
    "- Discuss the limitations and potential biases of the model\n",
    "\n",
    "For this task, relevant models to consider include logistic regression, naive Bayes, SVM, random forest, and BERT. You can compare their performance using the evaluation metrics you defined earlier. \n",
    "\n",
    "It's worth noting that BERT is a state-of-the-art model for NLP tasks, and it may require a different preprocessing pipeline than the other models. You might consider fine-tuning a pre-trained BERT model using transfer learning, which can improve its performance on your specific task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
